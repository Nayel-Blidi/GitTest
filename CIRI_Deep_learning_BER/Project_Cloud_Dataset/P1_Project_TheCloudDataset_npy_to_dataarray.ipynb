{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86777301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ PART I ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151aea1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "import ML\n",
    "\n",
    "#The cloud dataset, 6go, np arrays 30+go\n",
    "#https://www.kaggle.com/datasets/christianlillelund/the-cloudcast-dataset?resource=download\n",
    "\n",
    "#The cloud dataset : dataset of suboptimal npy arrays to dataarray\n",
    "\n",
    "#Data is structured in 24 files, for every month, named {year}M{month}\n",
    "#Every npy is the labelized cloud category, at t={num_array}*15 minutes\n",
    "#The data is studied above europe, centered on France, and covers some parts of the Maghreb\n",
    "#More especially between the long and lats : cf GEO.npz\n",
    "\n",
    "\"\"\"\n",
    "Specify the absolute path of the 'Project' folder location :\n",
    "\"\"\"\n",
    "abs_path = \"D:/Machine Learning/Project\"\n",
    "\n",
    "data_array_0 = np.load(abs_path+\"/The cloud dataset/2017M01/0.npy\")\n",
    "num_labels = np.max(data_array_0) - np.min(data_array_0) + 1\n",
    "plt.imshow(data_array_0, cmap='cool')\n",
    "\n",
    "lats = np.load(abs_path+\"/The cloud dataset/2017M01/GEO.npz\")['lats']\n",
    "lons = np.load(abs_path+\"/The cloud dataset/2017M01/GEO.npz\")['lons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5790ff3f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_mask_and_border(array, num_labels):\n",
    "    \n",
    "    m, n = array.shape\n",
    "    kernel = np.array([[-1, -1, -1], [-1,  8, -1], [-1, -1, -1]])\n",
    "\n",
    "    #Separation of the array masks by labels values\n",
    "    list_array_sampled = []\n",
    "    for k in range(1, num_labels+1):\n",
    "        array_sampled = array.copy()\n",
    "        array_sampled[array_sampled != k] = 0\n",
    "        list_array_sampled.append((array_sampled.copy()//k).astype('int8'))\n",
    "\n",
    "    #Convolution of the contours of every separated labels\n",
    "    list_array_sampled_training_border = []\n",
    "    for k in range(num_labels):\n",
    "        image = list_array_sampled[k]\n",
    "        border_image = convolve2d(image, kernel, mode='same', boundary='symm')\n",
    "        border_image[border_image < 0] = 0\n",
    "        border_image[border_image > 1] = 1\n",
    "        #border_image = (border_image - np.min(border_image)) / (np.max(border_image) - np.min(border_image))\n",
    "        list_array_sampled_training_border.append(border_image.astype('int8'))\n",
    "\n",
    "    return list_array_sampled, list_array_sampled_training_border\n",
    "\n",
    "\n",
    "def plot_mask_and_border(list_array_sampled, list_array_sampled_training_border, K=-1):\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=4, ncols=6, figsize=(16, 9))     \n",
    "    fig.subplots_adjust(hspace=0.6)\n",
    "    plt.suptitle(f\"Array {K}, separated labelized clouds and its borders\")\n",
    "    for k in tqdm(range(0, num_labels-1)):\n",
    "        plt.subplot(4, 6, k+1)\n",
    "        plt.imshow(list_array_sampled[k], cmap='cool')   \n",
    "        plt.title(f\"Separated label {k+1}\")\n",
    "        plt.subplot(4, 6, num_labels+k)\n",
    "        plt.imshow(list_array_sampled_training_border[k], cmap='cool')    \n",
    "        plt.title(f\"Border label {k+1}\")\n",
    "        \n",
    "    plt.savefig(abs_path+f\"/Machine Learning/Project/Separated_array_{K}.png\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30733966",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "list_array_sampled, list_array_sampled_training_border = compute_mask_and_border(data_array_0, num_labels)\n",
    "plot_mask_and_border(list_array_sampled, list_array_sampled_training_border, 0)\n",
    "\n",
    "# %%%\n",
    "\"\"\"\n",
    "Now this is done for one array, let's use this function to edit all the arrays of a folder.\n",
    "\n",
    "We'll then save all of these arrays in a more optimal extension, so all the data\n",
    "(new_size = original_size * 26)\n",
    "(new_size = (original_size=768Â²) * (num_labels=13) * (labels&borders=2)) \n",
    "can be loaded and stored more efficiently than as npy arrays\n",
    "    \n",
    "With all the data ready for learning, we'll start the neural network sorting processing in a dedicated file,\n",
    "so all of the data mining calculation doesn't have to be calculated again.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c3947",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_data_array(list_array, num_labels=13):\n",
    "    \n",
    "    data_array = xr.DataArray(np.stack(list_array), \n",
    "                              dims=('label', 'x', 'y'), \n",
    "                              coords={'label': range(num_labels)},\n",
    "                              name='data')\n",
    "    return data_array\n",
    "\n",
    "\n",
    "def save_data_array(path_in, path_out_labels, path_out_borders, name_darray, \n",
    "                    batch_start, batch_end, batch_size=96, batch_info=\"\", num_labels=13):\n",
    "    \n",
    "    try:\n",
    "    #Some month samples are sometimes slightly smaller than a full month, so in this case,\n",
    "    #we've choosed to don't store this data.\n",
    "    #Learning data will be marginally smaller, but for every instance, all the 26 different categories \n",
    "    #of the datasets will have the same shape, which is more valuable.\n",
    "            \n",
    "        #Dimension informations\n",
    "        timestamps = np.load(f\"{path_in}{name_darray}/TIMESTAMPS.npy\") \n",
    "        t = len(timestamps)\n",
    "    \n",
    "        #Initialisation of the dataaary with matrix 0\n",
    "        array = np.load(f\"{path_in}{name_darray}/{batch_start}.npy\")\n",
    "        \n",
    "        #Creation of 2 lists for matrix k :\n",
    "            #list_array_separated is the list of images with 1 label selected each\n",
    "            #list_array_separated_border is the list if the borders of each image\n",
    "        list_array_separated, list_array_separated_border = compute_mask_and_border(array, \n",
    "                                                                                    num_labels)\n",
    "        \n",
    "        #Conversion of both lists to data array, where label is the prime direction\n",
    "        concatenated_darray_separated = generate_data_array(list_array_separated)\n",
    "        concatenated_darray_separated_border = generate_data_array(list_array_separated_border)\n",
    "        \n",
    "        #The data arrays are concatenated for the whole folder\n",
    "        for k in tqdm(range(batch_start+1, batch_end)):\n",
    "            #Loading of matrix k\n",
    "            array = np.load(f\"{path_in}{name_darray}/{k}.npy\")\n",
    "            \n",
    "            #Creation of 2 lists for matrix k :\n",
    "                #list_array_separated is the list of images with 1 label selected each\n",
    "                #list_array_separated_border is the list if the borders of each image\n",
    "            list_array_separated, list_array_separated_border = compute_mask_and_border(array, \n",
    "                                                                                        num_labels)\n",
    "            \n",
    "            #Conversion of both lists to data array, where label=k is the prime direction\n",
    "            data_array_separated = generate_data_array(list_array_separated, num_labels)\n",
    "            data_array_separated_border = generate_data_array(list_array_separated_border, num_labels)\n",
    "            \n",
    "            #Concatenation\n",
    "            concatenated_darray_separated = xr.concat([concatenated_darray_separated, \n",
    "                                                       data_array_separated], dim='time')\n",
    "            concatenated_darray_separated_border = xr.concat([concatenated_darray_separated_border, \n",
    "                                                       data_array_separated_border], dim='time')\n",
    "\n",
    "        #Saving of the new dataarrays\n",
    "        concatenated_darray_separated.to_netcdf(f\"{path_out_labels}/{name_darray}_concatenated_darray_separated{batch_info}.nc\", \n",
    "                                                mode='w', format='netCDF4', engine='netcdf4', encoding={'data': {'zlib': True}})\n",
    "        concatenated_darray_separated_border.to_netcdf(f\"{path_out_borders}/{name_darray}_concatenated_darray_separated_borders{batch_info}.nc\", \n",
    "                                                       mode='w', format='netCDF4', engine='netcdf4', encoding={'data': {'zlib': True}})\n",
    "    \n",
    "    #Let's just not add anything to our\n",
    "    except:\n",
    "        return None, None\n",
    "        \n",
    "    return concatenated_darray_separated, concatenated_darray_separated_border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211d08f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "path_in = abs_path+\"/The cloud dataset/\"\n",
    "path_out_labels = abs_path+\"/TheCloudDataset_labelized_arrays_train\"\n",
    "path_out_borders = abs_path+\"/TheCloudDataset_labelized_borders_train\"\n",
    "name_darray = \"2017M01\"\n",
    "\n",
    "dataarray_labels, dataarray_borders = save_data_array(path_in, path_out_labels, path_out_borders, name_darray,\n",
    "                                                     batch_start=0, batch_end=96, num_labels=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465c390",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "#Due to performance issue, dataarray had to be batched\n",
    "def batched_save_data_array(path_in, path_in_labels, path_out_borders, name_darray, \n",
    "                            batch_size=96, num_labels=13):\n",
    "    \n",
    "    #Dimension informations\n",
    "    timestamps = np.load(f\"{path_in}{name_darray}/TIMESTAMPS.npy\") \n",
    "    t = len(timestamps)\n",
    "    \n",
    "    #Batch size is 1 day, over 31 days in the case of January, 28 for February... \n",
    "    for k in tqdm(range(0, t-batch_size, batch_size)):\n",
    "        #Every dataarray is by default a batch of (1 day = 1*24*4 = 96), 15min time intervals\n",
    "        batch_info = f\"_{k//batch_size}\"\n",
    "        batch_start = k\n",
    "        batch_end = k+batch_size\n",
    "        dataarray_labels, dataarray_borders = save_data_array(path_in, path_out_labels, path_out_borders, name_darray,\n",
    "                                                             batch_start, batch_end, batch_size, batch_info, 13)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae78e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = abs_path+\"/The cloud dataset/\"\n",
    "path_out_labels = abs_path+\"/TheCloudDataset_labelized_arrays_train\"\n",
    "path_out_borders = abs_path+\"/TheCloudDataset_labelized_borders_train\"\n",
    "name_darray = \"2017M01\"\n",
    "\n",
    "#Computation takes roughly 20min for a month\n",
    "batched_save_data_array(path_in, path_out_labels, path_out_borders, name_darray,\n",
    "                        batch_size=96, num_labels=13)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d54c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = abs_path+\"/The cloud dataset/\"\n",
    "path_out_labels = abs_path+\"/TheCloudDataset_labelized_arrays_train\"\n",
    "path_out_borders = abs_path+\"/TheCloudDataset_labelized_borders_train\"\n",
    "\n",
    "#Dirty loop over the whole year 2017\n",
    "#REFAIRE M02 0-4\n",
    "for k in range(7, 11):\n",
    "    if k <= 8:\n",
    "        name_darray = f\"2017M0{k+1}\"\n",
    "        batched_save_data_array(path_in, path_out_labels, path_out_borders, name_darray,\n",
    "                                batch_size=96, num_labels=13)  \n",
    "    elif k >= 9:\n",
    "        name_darray = f\"2017M{k+1}\"\n",
    "        batched_save_data_array(path_in, path_out_labels, path_out_borders, name_darray,\n",
    "                                batch_size=96, num_labels=13)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c116ac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data is now packed in 12 forlders over the year 2017, as netcdf arrays\n",
    "\n",
    "To sumarize the compression, let's take the example of one day in January 2017:\n",
    "    One day in npy format was 24*4*577kb = 55392kb\n",
    "    Now, for one day, there are 26 time more values (13 labels, 2 categories), and\n",
    "    the file size for one array is 18577 + 20456 = 39033kb\n",
    "    42% larger, yet storing 26 times less data\n",
    "    \n",
    "It is now possible to go on the next step, and start the learning of the cloud patterns\n",
    "(might regroup all the data in 2 large netcdf dataarrays)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_labels = abs_path+\"/TheCloudDataset_labelized_arrays_train\"\n",
    "path_borders = abs_path+\"/TheCloudDataset_labelized_borders_train\"\n",
    "\n",
    "def merge_data_array(path_labels = abs_path+\"/TheCloudDataset_labelized_arrays_train\",\n",
    "                     path_borders = abs_path+\"/TheCloudDataset_labelized_borders_train\"):\n",
    "    \n",
    "    #Merging of all the daily file into one full year dataset\n",
    "    files_darray_labels = [os.path.join(path_labels, f) for f in os.listdir(path_labels) if f.endswith(\".nc\")]\n",
    "    files_darray_borders = [os.path.join(path_borders, f) for f in os.listdir(path_borders) if f.endswith(\".nc\")]\n",
    "\n",
    "    #merged_darray_labels = xr.open_mfdataset(files_darray_labels, combine='nested', concat_dim=\"time\")\n",
    "    #merged_darray_borders = xr.open_mfdataset(files_darray_borders, combine='nested', concat_dim=\"time\")\n",
    "    \n",
    "    merged_darray_labels = xr.open_dataset(files_darray_labels[0])\n",
    "    for k in tqdm(range(1, len(files_darray_labels)-200)):\n",
    "        array = xr.open_dataset(files_darray_labels[k])\n",
    "        merged_darray_labels = xr.concat([merged_darray_labels, array], dim=\"time\")\n",
    "    merged_darray_labels.to_netcdf(f\"{path_labels}/2017M00_merged_darray_labels.nc\")\n",
    "\n",
    "    merged_darray_borders = xr.open_dataset(files_darray_labels[0])\n",
    "    for k in tqdm(range(1, len(files_darray_borders)-238)):\n",
    "        array = xr.open_dataset(files_darray_borders[k])\n",
    "        merged_darray_borders = xr.concat([merged_darray_borders, array], dim=\"time\")\n",
    "    merged_darray_borders.to_netcdf(f\"{path_borders}/2017M00_merged_darray_borders.nc\")\n",
    "\n",
    "    return merged_darray_labels, merged_darray_borders\n",
    "\n",
    "merged_darray_labels, merged_darray_borders = merge_data_array(path_labels, path_borders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a05b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_labels = abs_path+\"/TheCloudDataset_labelized_arrays_train\"\n",
    "path_borders = abs_path+\"/TheCloudDataset_labelized_borders_train\"\n",
    "\n",
    "def merge_data_array_bis(path_labels = abs_path+\"/TheCloudDataset_labelized_arrays_train\",\n",
    "                             path_borders = abs_path+\"/TheCloudDataset_labelized_borders_train\"):\n",
    "    \n",
    "    #Merging of all the daily file into one full year \n",
    "    files_darray_labels = [os.path.join(path_labels, f) for f in os.listdir(path_labels) if f.endswith(\".nc\")]\n",
    "    files_darray_borders = [os.path.join(path_borders, f) for f in os.listdir(path_borders) if f.endswith(\".nc\")]\n",
    "    \n",
    "    # open the files using dask\n",
    "    merged_darray_labels = xr.open_mfdataset(files_darray_labels, combine='nested', concat_dim='time', engine='netcdf4', chunks={'time': 'auto'})\n",
    "    merged_darray_borders = xr.open_mfdataset(files_darray_borders, combine='nested', concat_dim='time', engine='netcdf4', chunks={'time': 'auto'})\n",
    "\n",
    "    # save the concatenated dataset to a new netCDF file\n",
    "    merged_darray_labels.to_netcdf(f\"{path_labels}/2017M00_merged_darray_labels.nc\", mode='w')\n",
    "    merged_darray_borders.to_netcdf(f\"{path_borders}/2017M00_merged_darray_borders.nc\", mode='w')\n",
    "\n",
    "    return merged_darray_labels, merged_darray_borders\n",
    "\n",
    "merged_darray_labels, merged_darray_borders = merge_data_array_bis(path_labels, path_borders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288fe911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ END OF PART I ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9cb3a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
