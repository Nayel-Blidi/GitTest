{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ PART II ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ccf7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from math import trunc\n",
    "\n",
    "import ML\n",
    "\n",
    "#The cloud dataset, 6go, np arrays 30+go\n",
    "#https://www.kaggle.com/datasets/christianlillelund/the-cloudcast-dataset?resource=download\n",
    "\n",
    "#The cloud dataset : dataset of suboptimal npy arrays to dataarray\n",
    "\n",
    "#Edited dataset after npy_to_dataarray.py computation\n",
    "#Data is now structured in 2 datasets for learning :\n",
    "    #merged_darray_labels regroups the masked array of every cloud images by labels\n",
    "    #Shapes info :\n",
    "        #Time : roughly 365*24*4/15 files, ie one image every 15min\n",
    "        #Label : the type of cloud, defined by its altitude\n",
    "        #x : longitude coordinate\n",
    "        #y : lattitude coordinates\n",
    "    \n",
    "    #merged_darray_borders regroups the borders of every cloud images by labels\n",
    "    #Shapes info :\n",
    "        #Time : roughly 365*24*4/15 files, ie one image every 15min (0-)\n",
    "        #Label : the type of cloud, defined by its altitude (0-12)\n",
    "        #x : longitude coordinate (0-767)\n",
    "        #y : lattitude coordinates (0-767)\n",
    "    \n",
    "\"\"\"\n",
    "Specify the absolute path of the 'Project' folder location :\n",
    "\"\"\"\n",
    "abs_path = \"D:/Machine Learning/Project\"\n",
    "\n",
    "path_labels = abs_path+\"/TheCloudDataset_labelized_arrays_train\"\n",
    "path_borders = abs_path+\"/TheCloudDataset_labelized_borders_train\"\n",
    "\n",
    "files = [os.path.join(path_labels, f) for f in os.listdir(path_labels) if f.endswith(\".nc\")]\n",
    "array = np.load(abs_path+\"/The cloud dataset/2017M01/0.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,8)\n",
    "for k in range(32):\n",
    "    if k<=15:\n",
    "        plt.subplot(4,8,k+1)\n",
    "        plt.imshow(np.load(abs_path+f\"/The cloud dataset/2017M01/{k*96}.npy\"), cmap='cool')\n",
    "        plt.title(f\"{k+1}/01/2017\")\n",
    "    if k<=15:\n",
    "        plt.subplot(4,8,k+17)\n",
    "        plt.imshow(np.load(abs_path+f\"/The cloud dataset/2018M01/{k*96}.npy\"), cmap='cool')\n",
    "        plt.title(f\"{k+1}/01/2018\")\n",
    "\n",
    "plt.get_current_fig_manager().full_screen_toggle()\n",
    "plt.savefig(abs_path+\"/daily_img_2017_2018.png\")\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be88160",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The learning will follow these steps :\n",
    "    1 - PCA over every image to reduce its size and thus computation time\n",
    "    2 - 1vsAll algorithm over labels and borders\n",
    "    2bis - 1vsAll testing with the 2018 dataset (might merge it as well)\n",
    "    3 - NN over labels and borders\n",
    "    3b - NN testing with the 2018 dataset (might merge it as well)\n",
    "    4 - comparison of the algorithm results\n",
    "    4b - testin of the two algorithm over raw input (live images from the EUMETSAT)\n",
    "    5 - trying to improve the results by combining labels and borders when learning\n",
    "    5b - testing over 2018 dataset\n",
    "    5c - testing over raw input (live images from the EUMETSAT)\n",
    "    \n",
    "Depending on the difficulty, file size and computation time, every step may not be acheived, if not sucessfull.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61491995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ 1 - PCA ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89951cb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def slice_array(array, sclices=6):\n",
    "    \n",
    "    arr_blur_reduced = array[::sclices, ::sclices]\n",
    "    \n",
    "    return arr_blur_reduced\n",
    "    \n",
    "\n",
    "def pca_array(array, k_main_components, normalization=False):\n",
    "    \n",
    "    array_pca, pca_info = ML.pca(array, k_main_components, normalization)\n",
    "    \n",
    "    return array_pca, pca_info\n",
    "\n",
    "\n",
    "def pca_recoverArray(array_pca, pca_info):\n",
    "    \n",
    "    array_recover = ML.pca_recoverData(array_pca, pca_info)\n",
    "    \n",
    "    return array_recover\n",
    "\n",
    "\n",
    "def compute_pca_dataset(dataset, k_main_components=50):\n",
    "    \n",
    "    dataarray_pca = dataset['data'].values.astype(np.float32)\n",
    "    len_time, len_label, m, n = dataarray_pca.shape\n",
    "    \n",
    "    m, n = m//6, n//6\n",
    "    \n",
    "    flattened_pca_array = np.zeros((len_label, len_time//8, m*k_main_components))\n",
    "    for t in tqdm(range(0, len_time, 8)):\n",
    "        for l in range(len_label):\n",
    "            try:\n",
    "                array_pca = pca_array(slice_array(dataarray_pca[t,l,:,:]), k_main_components)[0]\n",
    "            except:\n",
    "                array_pca = np.zeros((m, k_main_components))\n",
    "            flattened_pca_array[l, t//8, :] = array_pca.flatten()\n",
    "    \n",
    "    return flattened_pca_array\n",
    "\n",
    "\n",
    "def save_pca_flattened_dataset(path_in, path_out, k_main_components=50):\n",
    "    \n",
    "    files = [os.path.join(path_in, f) for f in os.listdir(path_in) if f.endswith(\".nc\")]\n",
    "\n",
    "    for k in tqdm(range(len(files))):\n",
    "        dataset = xr.open_dataset(f\"{files[k]}\")\n",
    "        flattened_pca_array = compute_pca_dataset(dataset, k_main_components)\n",
    "        flattened_pca_array = xr.DataArray(flattened_pca_array, dims=('time', 'data_pca', 'label'))\n",
    "        flattened_pca_array.to_netcdf(f\"{path_out}/pca_k{k_main_components}_{k}.nc\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe48de2f",
   "metadata": {
    "title": "TEST COMPUTE PCA"
   },
   "outputs": [],
   "source": [
    "dataset = xr.open_dataset(abs_path+\"/TheCloudDataset_labelized_arrays_train/2017M01_concatenated_darray_separated_0.nc\")\n",
    "array_pca = compute_pca_dataset(dataset, k_main_components=10)\n",
    "darray_pca = xr.DataArray(array_pca, dims=('time', 'data_pca', 'label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa51f8b",
   "metadata": {
    "title": "TEST PCA&RECOVER ARRAY"
   },
   "outputs": [],
   "source": [
    "array = xr.open_dataset(abs_path+\"/TheCloudDataset_labelized_arrays_train/2017M01_concatenated_darray_separated_0.nc\")['data'].values[0,0,:,:] \n",
    "array = slice_array(array)\n",
    "\n",
    "array_pca, array_pca_info = pca_array(array, k_main_components=30, normalization=True)\n",
    "array_pca_recovered = pca_recoverArray(array_pca, array_pca_info)\n",
    "\n",
    "l = [array, array_pca, array_pca_recovered]\n",
    "for k in range(3):\n",
    "    plt.subplot(1,3,k+1)\n",
    "    plt.imshow(l[k], cmap='gray')\n",
    "plt.suptitle(\"Image 0 label 0 / PCA img / PCA img recovered\")\n",
    "plt.savefig(abs_path+\"/1b_pca_example_k30.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ec8fa",
   "metadata": {
    "title": "TEST PCA&RECOVERS BORDER"
   },
   "outputs": [],
   "source": [
    "array = xr.open_dataset(abs_path+\"/TheCloudDataset_labelized_borders_train/2017M01_concatenated_darray_separated_borders_0.nc\")['data'].values[0,0,:,:] \n",
    "\n",
    "array_pca, array_pca_info = pca_array(array, k_main_components=50)\n",
    "array_pca_recovered = pca_recoverArray(array_pca, array_pca_info)\n",
    "\n",
    "l = [array, array_pca, array_pca_recovered]\n",
    "for k in range(3):\n",
    "    plt.subplot(1,3,k+1)\n",
    "    plt.imshow(l[k], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ff9c6",
   "metadata": {
    "title": "PCA COMPUTATION ARRAYS"
   },
   "outputs": [],
   "source": [
    "k_main_components = 20\n",
    "path_in = abs_path+\"/TheCloudDataset_labelized_arrays_train\"\n",
    "path_out = abs_path+f\"/TheCloudDataset_labelized_arrays_train_pca_k{k_main_components}\"\n",
    "\n",
    "#Dirty fonction calling to store the computed pca datasets\n",
    "save_pca_flattened_dataset(path_in, path_out, k_main_components) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928786b",
   "metadata": {
    "title": "PCA COMPUTATION BORDERS"
   },
   "outputs": [],
   "source": [
    "k_main_components = 10\n",
    "path_in = abs_path+\"/TheCloudDataset_labelized_borders_train\"\n",
    "path_out = abs_path+\"/TheCloudDataset_labelized_borders_train_pca_k{k_main_components}\"\n",
    "\n",
    "#Dirty fonction calling to store the computed pca datasets\n",
    "save_pca_flattened_dataset(path_in, path_out, k_main_components)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66915110",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"                   \n",
    "Over the extremely high complexity and time consumption of the computation, choices had to be made :\n",
    "(pca computation and writing duration was ~6h)\n",
    "    1) The PCA had to be very rough, with 10 k main components selected (might try 20-30 afterwards)\n",
    "    2) Data had to be reduced again : instead of 15min slices, it's now 2 hours. The impact should be small\n",
    "    enough over the futur learning, and it's still better than taking instead only 2.5 months of the whole year\n",
    "    instead (for the same data weight reduction) and thus missing the seasons variations\n",
    "    3) Data is now stored as non-compressed float 32, which should speed up the learning computation\n",
    "    4) Data is still stored as individual daily arrays, as merging is still to heavy :\n",
    "        - As for the previous compression, there are still too many files to load in the memory\n",
    "        - Lazy computation may work better on uncompressed data, but might be counter effective when\n",
    "        computing the neural network learning.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86fb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ 2 - OneVsAll ALGORITHM ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c81ff2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_toarray_selection(path_in, variable, sample_size=324, num_labels=13):\n",
    "    \n",
    "    files = [os.path.join(path_in, f) for f in os.listdir(path_in) if f.endswith(\".nc\")]\n",
    "    \n",
    "    #To check the length of a vector of an image (may change depending on the PCA)\n",
    "    data_info = xr.open_dataset(files[0])[variable].values\n",
    "    len_data = data_info.shape[-1]\n",
    "    len_data_daily = data_info.shape[-2]\n",
    "    \n",
    "    y = np.zeros((sample_size*len_data_daily))\n",
    "    X = np.zeros((sample_size*len_data_daily, len_data))\n",
    "    \n",
    "    print(X.shape)\n",
    "    \n",
    "    for t in tqdm(range(sample_size*len_data_daily)):\n",
    "        X[t, :] = xr.open_dataset(files[t//len_data_daily])[variable].values[t%num_labels, t%len_data_daily, :].flatten()\n",
    "        y[t] = t%num_labels\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def OneVsAll_dataset(path_in, variable, sample_size=324, lambda_=1, num_labels=13, itterations=10):\n",
    "    \n",
    "    print(\"Loading of X and y\")\n",
    "    X, y = data_toarray_selection(path_in, variable, sample_size, num_labels)\n",
    "    print(\"X and y finished loading\")\n",
    "    \n",
    "    trained_theta = ML.oneVsAll(X, y, num_labels, lambda_, tol=1e-3)[1]\n",
    "    \n",
    "    return trained_theta, X, y\n",
    "\n",
    "\n",
    "def OneVsAll_testDataset_pca(path_in, trained_theta, variable, sample_size=30, num_labels=13):\n",
    "    \n",
    "    files = [os.path.join(path_in, f) for f in os.listdir(path_in) if f.endswith(\".nc\")]\n",
    "    \n",
    "    #To check the length of a vector of an image (may change depending on the PCA)\n",
    "    data_info = xr.open_dataset(files[0])[variable].values\n",
    "    len_data = data_info.shape[-1]\n",
    "    len_data_daily = data_info.shape[-2]\n",
    "    \n",
    "    X = np.zeros((sample_size*len_data_daily, len_data))\n",
    "    for t in tqdm(range(sample_size*len_data_daily)):\n",
    "        X[t, :] = xr.open_dataset(files[t//len_data_daily])[variable].values[t%num_labels, t%len_data_daily].flatten()\n",
    "    \n",
    "    p = ML.predictOneVsAll(trained_theta[:,1:], X)\n",
    "    \n",
    "    for k in range(num_labels):\n",
    "        success_of_label_k = 0\n",
    "        for t in range(k, sample_size*len_data_daily, num_labels):\n",
    "            if p[t] == k:\n",
    "                success_of_label_k+=1\n",
    "        success_of_label_k = success_of_label_k / (sample_size*len_data_daily/num_labels)\n",
    "        print(\"Success of label\", k, \"is : \", success_of_label_k*100, \"%\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def OneVsAll_testArray(path_in, trained_theta, variable, sample_size=30, num_labels=13, k_main_components=10, slices=6):\n",
    "    \n",
    "    files = [os.path.join(path_in, f) for f in os.listdir(path_in) if f.endswith(\".npy\")]\n",
    "    size_array_pca = np.load(files[0]).shape[0]*k_main_components\n",
    "    \n",
    "    X = np.zeros((sample_size*num_labels, size_array_pca//slices))\n",
    "    for t in tqdm(range(sample_size*num_labels)):\n",
    "        array = np.load(files[t//num_labels])\n",
    "        array[array != (t+1)%num_labels] = 0\n",
    "        array[array == (t+1)%num_labels] = 1\n",
    "        array_pca = pca_array(slice_array(array), k_main_components)[0]\n",
    "        X[t, :] = array_pca.flatten()\n",
    "    \n",
    "    p = ML.predictOneVsAll(trained_theta[:,1:], X)\n",
    "    \n",
    "    for k in range(num_labels):\n",
    "        success_of_label_k = 0\n",
    "        for t in range(k, sample_size*num_labels, num_labels):\n",
    "            if p[t] == k%num_labels:\n",
    "                success_of_label_k+=1\n",
    "        success_of_label_k = success_of_label_k / sample_size\n",
    "        print(\"Success of label\", k, \"is : \", success_of_label_k*100, \"%\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def plot_OneVsAll_testDataset(path_in, trained_theta, p, sample_size=30, subplot_size=3, num_labels=13):\n",
    "    \n",
    "    files = [os.path.join(path_in, f) for f in os.listdir(path_in) if f.endswith(\".nc\")]\n",
    "    \n",
    "    len_subplot = subplot_size**2\n",
    "    \n",
    "    fig, ax = plt.subplots(subplot_size, subplot_size)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    for t in tqdm(range(len_subplot)):\n",
    "        array = xr.open_dataset(files[t//num_labels])['data'].values[t//num_labels, t%num_labels, :, :]\n",
    "        \n",
    "        try:\n",
    "            plt.subplot(subplot_size, subplot_size, t+1)\n",
    "            plt.imshow(array)\n",
    "            plt.title(f\"Real label is : {t%num_labels} \\n Predicted label is : {p[t]}\")\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "    plt.savefig(abs_path+f\"/OneVsAll_2017_k10.png\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c52285",
   "metadata": {
    "K": 10,
    "lines_to_next_cell": 2,
    "title": "1VSALL ARRAYS"
   },
   "outputs": [],
   "source": [
    "path_in_arrays_pca = abs_path+\"/TheCloudDataset_labelized_arrays_train_pca_k10\"\n",
    "#If dataset is PCA, data variable is '__xarray_dataarray_variable__', 'data' otherwise\n",
    "variable = \"__xarray_dataarray_variable__\"\n",
    "\n",
    "trained_theta_arrays, X_arrays, y_arrays = OneVsAll_dataset(path_in_arrays_pca, variable, sample_size=324,\n",
    "                                                            lambda_=1, num_labels=13, itterations=30)\n",
    "\n",
    "np.save(abs_path+\"/trained_theta_k20_itt30_sample324.npy\", trained_theta_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6459e1b",
   "metadata": {
    "K": 20,
    "title": "1VSALL ARRAYS"
   },
   "outputs": [],
   "source": [
    "path_in_arrays_pca = abs_path+\"/TheCloudDataset_labelized_arrays_train_pca_k10\"\n",
    "#If dataset is PCA, data variable is '__xarray_dataarray_variable__', 'data' otherwise\n",
    "variable = \"__xarray_dataarray_variable__\"\n",
    "\n",
    "trained_theta_arrays, X_arrays, y_arrays = OneVsAll_dataset(path_in_arrays_pca, variable, sample_size=324,\n",
    "                                                            lambda_=1, num_labels=13, itterations=30)\n",
    "\n",
    "p_arrays = OneVsAll_testDataset_pca(path_in_arrays_pca, trained_theta_arrays, variable,\n",
    "                                    sample_size=1, num_labels=13)\n",
    "\n",
    "plot_OneVsAll_testDataset(path_in_arrays_pca, trained_theta_arrays, p_arrays, variable, \n",
    "                          sample_size=30, subplot_size= 6, num_labels=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d5027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "New choices had to be made : even when reducing PCA to 10 main components vectors, the output array\n",
    "size is 728*10=7280 \"pixels\" to study, which way too long for scipy.optimize.minimize.\n",
    "Even when sampling ony the first month, the learning matrix was 360*7280.\n",
    "Thus, we had a new idea : to first reduce the images size before computing both the learning \n",
    "algorithm and the PCA of the data.\n",
    "Thus, for 10 main components, thetas calculation was ~16min, and 1h10min for k=20.\n",
    "In any way, we still mainly hope for the to-be-done neural network to converge more quickly and be\n",
    "able to handle all the data\n",
    "\n",
    "PS : array slicing gave bad results on the convoluted border dataset. Convolution product should \n",
    "be done over the newly sliced data.\n",
    "PS2 : tol in scipy.optimize.minimize was set to 10e-3 instead of the default 10e-8 considering \n",
    "the values of the arrays being aroung 10e1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf71452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ 2b - OneVsAll ALGORITHM TESTING ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55374f20",
   "metadata": {
    "incorrectly_encoded_metadata": "K=10 2017 (TRAINED)",
    "title": "TESTING"
   },
   "outputs": [],
   "source": [
    "trained_theta_arrays = np.load(abs_path+\"/trained_theta_k10_itt30_sample324.npy\")\n",
    "path_in_arrays_pca = abs_path+\"/TheCloudDataset_labelized_arrays_train_pca_k10\"\n",
    "variable = \"__xarray_dataarray_variable__\"\n",
    "\n",
    "p_arrays = OneVsAll_testDataset_pca(path_in_arrays_pca, trained_theta_arrays, variable,\n",
    "                                sample_size=324, num_labels=13)\n",
    "\n",
    "path_in_arrays = abs_path+\"/TheCloudDataset_labelized_arrays_train\"\n",
    "\n",
    "plot_OneVsAll_testDataset(path_in_arrays, trained_theta_arrays, p_arrays, \n",
    "                          sample_size=324, subplot_size=4, num_labels=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f7220",
   "metadata": {
    "incorrectly_encoded_metadata": "K=20 2017 (TRAINED)",
    "title": "TESTING"
   },
   "outputs": [],
   "source": [
    "trained_theta_arrays = np.load(abs_path+\"/trained_theta_k20_itt30_sample324.npy\")\n",
    "path_in_arrays_pca = abs_path+\"/TheCloudDataset_labelized_arrays_train_pca_k20\"\n",
    "variable = \"__xarray_dataarray_variable__\"\n",
    "\n",
    "p_arrays = OneVsAll_testDataset_pca(path_in_arrays_pca, trained_theta_arrays, variable,\n",
    "                                sample_size=324, num_labels=13)\n",
    "\n",
    "path_in_arrays = abs_path+\"/TheCloudDataset_labelized_arrays_train\"\n",
    "\n",
    "plot_OneVsAll_testDataset(path_in_arrays, trained_theta_arrays, p_arrays, \n",
    "                          sample_size=324, subplot_size=4, num_labels=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc866e",
   "metadata": {
    "incorrectly_encoded_metadata": "K=10 2018 (TEST)",
    "title": "TESTING"
   },
   "outputs": [],
   "source": [
    "trained_theta_arrays = np.load(abs_path+\"/trained_theta_k10_itt30_sample324.npy\")\n",
    "path_in_arrays_2018 = abs_path+\"/The cloud dataset/2018M10\"\n",
    "variable = \"__xarray_dataarray_variable__\"\n",
    "\n",
    "p_arrays = OneVsAll_testArray(path_in_arrays_2018, trained_theta_arrays, variable,\n",
    "                                sample_size=1000, num_labels=13, k_main_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f04f8",
   "metadata": {
    "incorrectly_encoded_metadata": "K=20 2018 (TEST)",
    "title": "TESTING"
   },
   "outputs": [],
   "source": [
    "trained_theta_arrays = np.load(abs_path+\"/trained_theta_k20_itt30_sample324.npy\")\n",
    "path_in_arrays_2018 = abs_path+\"/The cloud dataset/2018M11\"\n",
    "variable = \"__xarray_dataarray_variable__\"\n",
    "\n",
    "p_arrays = OneVsAll_testArray(path_in_arrays_2018, trained_theta_arrays, variable,\n",
    "                                sample_size=500, num_labels=13, k_main_components=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4493dbb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing over the whole 2017 folder :\n",
    "When testing with k=20 over the learning dataset, results are excellent exepted for 3 categories :\n",
    "labels 2, 3 and 8. This tremendous lack of performance comes from a lack of data, as occurences of \n",
    "those labels are very rare. They then should be removed from the learning examples, or be trained on \n",
    "a dedicated dataset, with more examples (most of the arrays are null)\n",
    "\n",
    "PS : error calculation is sometimes slightly above 100%, depending on the rounding of the size of the folder\n",
    "\n",
    "Testing over the whole January 2018 folder :\n",
    "When testing with k=10 and k=20 over testing dataset, results are overwhelmingly disapointing.\n",
    "Actually, it recognizes consistantly some labels but doesn't attribute it the right value. \n",
    "(eg : label 4 is always recognised as 0)\n",
    "The testing has to be realised over larger sets to confirm the biaises. If confirmed, then the finesse\n",
    "of the algorithm should be reduced, to help breaking those biaises.\n",
    "\n",
    "\n",
    "Testing over the whole 2018 folder :\n",
    "Biaises seems to be the same. Leet's keep going by woorking on the neural network before coming back\n",
    "here to merge some categories. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899636f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ 3 - NEURAL NETWORK ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f140b19",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_datasetNN(path_in, variable=\"__xarray_dataarray_variable__\", sample_size=100, hidden_layer_size=32, num_labels=13, itterations=100):\n",
    "    \n",
    "    files = [os.path.join(path_in, f) for f in os.listdir(path_in) if f.endswith(\".nc\")]\n",
    "    \n",
    "    #To check the length of a vector of an image (may change depending on the PCA)\n",
    "    data_info = xr.open_dataset(files[0])[variable].values\n",
    "    len_data = data_info.shape[-1]\n",
    "    len_data_daily = data_info.shape[-2]\n",
    "    \n",
    "    X, y = data_toarray_selection(path_in, variable, sample_size, num_labels)\n",
    "    \n",
    "    Theta1, Theta2 = ML.nnOneLayer(X, y, hidden_layer_size, num_labels, itterations)\n",
    "        \n",
    "    return Theta1, Theta2\n",
    "\n",
    "\n",
    "def test_datasetNN(path_in, Theta1, Theta2, sample_size=100, num_labels=13):\n",
    "    \n",
    "    X, y = data_toarray_selection(path_in, variable, sample_size, num_labels)\n",
    "    p_arrays = ML.predOneLayer(X, Theta1, Theta2)\n",
    "    \n",
    "    len_sample, len_data = X.shape\n",
    "    print(X.shape)\n",
    "    \n",
    "    for k in range(num_labels):\n",
    "        success_of_label_k = 0\n",
    "        for t in range(k, len_sample, num_labels):\n",
    "            if p_arrays[t] == k:\n",
    "                success_of_label_k+=1\n",
    "        success_of_label_k = trunc(success_of_label_k/(len_sample/num_labels)*10000)\n",
    "        print(\"Success of label\", k, \"is : \", success_of_label_k/100, \"%\")\n",
    "\n",
    "    return p_arrays\n",
    "\n",
    "\n",
    "def test_arrayNN(path_in, Theta1, Theta2, k_main_components=10, sample_size=100, num_labels=13):\n",
    "    \n",
    "    files = [os.path.join(path_in, f) for f in os.listdir(path_in) if f.endswith(\".npy\")]\n",
    "    size_array_pca = np.load(files[0]).shape[0]*k_main_components\n",
    "    \n",
    "    X = np.zeros((sample_size*num_labels, size_array_pca//6))\n",
    "    for t in tqdm(range(sample_size*num_labels)):\n",
    "        array = np.load(files[t//num_labels])\n",
    "        array[array != (t+1)%num_labels] = 0\n",
    "        array[array == (t+1)%num_labels] = 1\n",
    "        array_pca = pca_array(slice_array(array), k_main_components)[0]\n",
    "        X[t, :] = array_pca.flatten()\n",
    "    \n",
    "    p_arrays = ML.predOneLayer(X, Theta1, Theta2)\n",
    "\n",
    "    len_sample, len_data = X.shape\n",
    "    \n",
    "    for k in range(num_labels):\n",
    "        success_of_label_k = 0\n",
    "        for t in range(k, len_sample, num_labels):\n",
    "            if p_arrays[t] == k:\n",
    "                success_of_label_k+=1\n",
    "        success_of_label_k = trunc(success_of_label_k/(len_sample/num_labels)*10000)\n",
    "        print(\"\", \"Success of label\", k, \"is : \", success_of_label_k/100, \"%\")\n",
    "\n",
    "    return p_arrays, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ecd613",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in_arrays = abs_path+\"/TheCloudDataset_labelized_arrays_train_pca_k10\"\n",
    "variable = \"__xarray_dataarray_variable__\"\n",
    "\n",
    "Theta1, Theta2 = run_datasetNN(path_in_arrays, variable, sample_size=324, num_labels=13, itterations=100)\n",
    "\n",
    "np.save(abs_path+\"/trained_NN_k10_itt100_sample324.npy\", np.array([Theta1, Theta2], dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in_arrays = abs_path+\"/TheCloudDataset_labelized_arrays_train_pca_k20\"\n",
    "variable = \"__xarray_dataarray_variable__\"\n",
    "\n",
    "Theta1, Theta2 = run_datasetNN(path_in_arrays, variable, sample_size=324, num_labels=13, itterations=100)\n",
    "\n",
    "np.save(abs_path+\"/trained_NN_k20_itt100_sample324.npy\", np.array([Theta1, Theta2], dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in_arrays = abs_path+\"/TheCloudDataset_labelized_arrays_train_pca_k30\"\n",
    "variable = \"__xarray_dataarray_variable__\"\n",
    "\n",
    "Theta1, Theta2 = run_datasetNN(path_in_arrays, variable, sample_size=324, num_labels=13, itterations=100)\n",
    "\n",
    "np.save(abs_path+\"/trained_NN_k30_itt100_sample324.npy\", np.array([Theta1, Theta2], dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62f1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ 3b - NEURAL NETWORK TESTING ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c7c7c1",
   "metadata": {
    "incorrectly_encoded_metadata": "K=10 2017 (TRAINED)",
    "title": "TESTING"
   },
   "outputs": [],
   "source": [
    "path_in_arrays = abs_path+\"/TheCloudDataset_labelized_arrays_train_pca_k10\"\n",
    "variable = \"__xarray_dataarray_variable__\"\n",
    "\n",
    "Theta1, Theta2 = np.load(abs_path+\"/trained_NN_k10_itt100_sample324.npy\", allow_pickle=True)\n",
    "\n",
    "p_arrays = test_datasetNN(path_in_arrays, Theta1, Theta2, sample_size=324, num_labels=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a691c8e",
   "metadata": {
    "incorrectly_encoded_metadata": "K=20 2017 (TRAINED)",
    "title": "TESTING"
   },
   "outputs": [],
   "source": [
    "path_in_arrays = abs_path+\"/TheCloudDataset_labelized_arrays_train_pca_k20\"\n",
    "variable = \"__xarray_dataarray_variable__\"\n",
    "\n",
    "Theta1, Theta2 = np.load(abs_path+\"/trained_NN_k20_itt100_sample324.npy\", allow_pickle=True)\n",
    "\n",
    "p_arrays = test_datasetNN(path_in_arrays, Theta1, Theta2, sample_size=324, num_labels=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8780d03",
   "metadata": {
    "incorrectly_encoded_metadata": "K=30 2017 (TRAINED)",
    "title": "TESTING"
   },
   "outputs": [],
   "source": [
    "path_in_arrays = abs_path+\"/TheCloudDataset_labelized_arrays_train_pca_k30\"\n",
    "variable = \"__xarray_dataarray_variable__\"\n",
    "\n",
    "Theta1, Theta2 = np.load(abs_path+\"/trained_NN_k20_itt100_sample324.npy\", allow_pickle=True)\n",
    "\n",
    "p_arrays = test_datasetNN(path_in_arrays, Theta1, Theta2, sample_size=324, num_labels=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8f28a",
   "metadata": {
    "incorrectly_encoded_metadata": "K=10 2018 (TEST)",
    "title": "TESTING"
   },
   "outputs": [],
   "source": [
    "path_in_arrays = \"D:/The cloud dataset/2018M01\"\n",
    "\n",
    "Theta1, Theta2 = np.load(\"D:/Machine Learning/Project/trained_NN_k10_itt100_sample324.npy\", allow_pickle=True)\n",
    "\n",
    "p_arrays, X = test_arrayNN(path_in_arrays, Theta1, Theta2, k_main_components=10, sample_size=1000,  num_labels=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36750b1b",
   "metadata": {
    "incorrectly_encoded_metadata": "K=20 2018 (TEST)",
    "title": "TESTING"
   },
   "outputs": [],
   "source": [
    "path_in_arrays = abs_path+\"/The cloud dataset/2018M01\"\n",
    "\n",
    "Theta1, Theta2 = np.load(abs_path+\"/trained_NN_k20_itt100_sample324.npy\", allow_pickle=True)\n",
    "\n",
    "p_arrays = test_arrayNN(path_in_arrays, Theta1, Theta2, k_main_components=20, sample_size=1000,  num_labels=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13fb414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ 4 - RESULTS ANALYSIS ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ccbdd1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The results are not satisfying enough to keep going. Some labels are (kinda) working whereas others\n",
    "are fuly broken (for multiple reasons).\n",
    "The way of improvement are as listed here :\n",
    "    1) Increasing the resolution of the PCA is not an option for the 1vsAll algorithm, as the maximum \n",
    "    computation time has already been reached. Altough, larger k main components values can be computed \n",
    "    for the Neural Network\n",
    "    2) Merging some labels can be planned, as clouds of a close categories are mainly flying in a pack\n",
    "    (cf images 0 and 1), and can add weight to a categorie when compared to an other.\n",
    "    3) Using the borders convolution to improve the accuracy, or maybe be the only dataset to learn from.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ceb056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ END OF PART II ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cdfac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "K,title,incorrectly_encoded_metadata,-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
