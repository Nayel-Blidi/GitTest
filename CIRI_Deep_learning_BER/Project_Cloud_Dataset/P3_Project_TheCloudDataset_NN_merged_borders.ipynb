{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a8f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ PART III ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import convolve2d\n",
    "from natsort import natsorted\n",
    "from math import trunc\n",
    "\n",
    "import ML\n",
    "\n",
    "#The cloud dataset, 6go, np arrays 30+go\n",
    "#https://www.kaggle.com/datasets/christianlillelund/the-cloudcast-dataset?resource=download\n",
    "#https://vision.eng.au.dk/cloudcast-dataset/\n",
    "\n",
    "#The cloud dataset : dataset of suboptimal npy arrays to dataarray\n",
    "\n",
    "#Data is structured in 24 files, for every month, named {year}M{month}\n",
    "#Every npy is the labelized cloud category, at t={num_array}*15 minutes\n",
    "#The data is studied above europe, centered on France, and covers some parts of the Maghreb\n",
    "#More especially between the long and lats : cf GEO.npz\n",
    "\n",
    "\"\"\"\n",
    "Specify the absolute path of the 'Project' folder location :\n",
    "\"\"\"\n",
    "abs_path = \"D:/Machine Learning/Project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fdeb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ CREATING (again) A NEW DATASET ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f4eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's merge the dataset by larger categories, as adjacent clouds usually belong to the same category\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d83bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,6)\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "for k in range(0, 96*4*6, 96):\n",
    "    array = np.load(f\"D:/The cloud dataset/2017M01/{k}.npy\")\n",
    "    plt.subplot(4,6, k//96+1)\n",
    "    plt.imshow(array, cmap='cool')\n",
    "    plt.title(f\"{k//96+1} January 2017\")\n",
    "#plt.get_current_fig_manager().full_screen_toggle()\n",
    "#plt.savefig(\"D:/Machine Learning/Project/10_daily_img_January_2017.png\")\n",
    "#plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ebefd2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "array = np.load(abs_path+\"/The cloud dataset/2017M01/0.npy\")\n",
    "plt.imshow(array, cmap='cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ee392",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def slice_array(array, slices=6):\n",
    "    \"\"\"\n",
    "    Divides the resolution of an input array by slices\n",
    "    \"\"\"\n",
    "    return array[::slices, ::slices]\n",
    "\n",
    "    \n",
    "def pca_array(array, k_main_components, normalization=True):\n",
    "    \"\"\"\n",
    "    Module fonction.\n",
    "    Runs PCA algorithm on an input array and keep the first k_main_components.\n",
    "    Input array can be normalized or not.\n",
    "    \"\"\"\n",
    "    array_pca, pca_info = ML.pca(array, k_main_components, normalization)\n",
    "    return array_pca, pca_info\n",
    "\n",
    "\n",
    "def merged_arrays(file, num_labels=4, slices=6, k_main_components=30):\n",
    "    \"\"\"\n",
    "    Merges all the arrays of a folder together, in 4 sub netcdf files :\n",
    "        1) Original sliced input array\n",
    "        2) PCA sliced input array\n",
    "        3) Convoluted sliced input array\n",
    "        4) PCA convoluted sliced array\n",
    "    \"\"\"\n",
    "    array = np.load(file)\n",
    "    array = slice_array(array, slices)\n",
    "    \n",
    "    m, n = array.shape\n",
    "    kernel = np.array([[-1, -1, -1], [-1,  8, -1], [-1, -1, -1]])\n",
    "    \n",
    "    new_array = np.zeros((num_labels, m, n))\n",
    "    new_array_pca = np.zeros((num_labels, m, k_main_components))\n",
    "    for k in range(num_labels):\n",
    "        array_label = np.zeros((m,n))\n",
    "        array_label[array==k*3+1] = 1\n",
    "        array_label[array==k*3+2] = 1\n",
    "        array_label[array==k*3+3] = 1\n",
    "        if k==0:\n",
    "            array_label[array==k] = 1 \n",
    "\n",
    "        new_array[k, :, :] = array_label\n",
    "        new_array_pca[k,:,:] = pca_array(array_label, k_main_components)[0]\n",
    "    \n",
    "    new_array_border = np.zeros((num_labels, m, n))\n",
    "    new_array_border_pca = np.zeros((num_labels, m, k_main_components))\n",
    "    for k in range(num_labels):\n",
    "        new_array_border[k,:,:] = convolve2d(new_array[k,:,:], kernel, mode='same', boundary='symm')\n",
    "        new_array_border_pca[k,:,:] = pca_array(new_array_border[k,:,:], k_main_components)[0]\n",
    "        \n",
    "    return new_array, new_array_pca, new_array_border, new_array_border_pca\n",
    "\n",
    "\n",
    "def save_dataset(path_in, path_out, year=\"2017\", num_labels=4, slices=6, k_main_components=30, compression=False):\n",
    "    \"\"\"\n",
    "    Save all the arrays in path_in folder into path_out folder, as merged datasets alongside dimension time.\n",
    "    Calls the function merged_arrays\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for k in range(1, 13):\n",
    "        if k <=9:\n",
    "            file_names = natsorted(os.listdir(abs_path+f\"/The cloud dataset/{year}M0{k}\"))\n",
    "            files += [os.path.join(f\"{path_in}/{year}M0{k}\", f) for f in file_names if f.endswith(\".npy\") and f != \"TIMESTAMPS.npy\"]\n",
    "        if k>=10:\n",
    "            file_names = natsorted(os.listdir(abs_path+f\"/The cloud dataset/{year}M{k}\"))\n",
    "            files += [os.path.join(f\"{path_in}/{year}M{k}\", f) for f in file_names if f.endswith(\".npy\") and f != \"TIMESTAMPS.npy\"]\n",
    "            \n",
    "    len_files = len(files)\n",
    "    list_new_array, list_new_array_pca, list_new_array_border, list_new_array_border_pca = [], [], [], []\n",
    "    for t in tqdm(range(0, len_files, 8)):\n",
    "        new_array, new_array_pca, new_array_border, new_array_border_pca = merged_arrays(files[t], num_labels, slices, k_main_components)\n",
    "        \n",
    "        list_new_array.append(xr.DataArray(new_array, dims=('label', 'x', 'y')))\n",
    "        list_new_array_pca.append(xr.DataArray(new_array_pca, dims=('label', 'x', 'y')))\n",
    "        list_new_array_border.append(xr.DataArray(new_array_border, dims=('label', 'x', 'y')))\n",
    "        list_new_array_border_pca.append(xr.DataArray(new_array_border_pca, dims=('label', 'x', 'y')))\n",
    "    \n",
    "    len_dataset = len(list_new_array)\n",
    "    \n",
    "    d1 = xr.DataArray(list_new_array, \n",
    "                 dims=('time', 'label', 'x', 'y'), \n",
    "                 coords={'time': range(len_dataset)},\n",
    "                 name='data')\n",
    "    d1.to_netcdf(f\"{path_out}/new_array.nc\", \n",
    "                            mode='w', format='netCDF4', engine='netcdf4', encoding={'data': {'zlib': compression}})\n",
    "    \n",
    "    d2 = xr.DataArray(list_new_array_pca, \n",
    "                 dims=('time', 'label', 'x', 'y'), \n",
    "                 coords={'time': range(len_dataset)},\n",
    "                 name='data')\n",
    "    d2.to_netcdf(f\"{path_out}/new_array_pca.nc\", \n",
    "                            mode='w', format='netCDF4', engine='netcdf4', encoding={'data': {'zlib': compression}})\n",
    "    \n",
    "    d3 = xr.DataArray(list_new_array_border, \n",
    "                 dims=('time', 'label', 'x', 'y'), \n",
    "                 coords={'time': range(len_dataset)},\n",
    "                 name='data')\n",
    "    d3.to_netcdf(f\"{path_out}/new_array_border.nc\", \n",
    "                            mode='w', format='netCDF4', engine='netcdf4', encoding={'data': {'zlib': compression}})\n",
    "    \n",
    "    d4 = xr.DataArray(list_new_array_border_pca, \n",
    "                 dims=('time', 'label', 'x', 'y'), \n",
    "                 coords={'time': range(len_dataset)},\n",
    "                 name='data')\n",
    "    d4.to_netcdf(f\"{path_out}/new_array_border_pca.nc\", \n",
    "                            mode='w', format='netCDF4', engine='netcdf4', encoding={'data': {'zlib': compression}})\n",
    "    \n",
    "    return new_array, new_array_pca, new_array_border, new_array_border_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = abs_path+\"/The cloud dataset\"\n",
    "path_out = abs_path+\"/P3_Training\"\n",
    "new_array, new_array_pca, new_array_border, new_array_border_pca = save_dataset(path_in, path_out, year=\"2017\", num_labels=4, slices=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1808da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = abs_path+\"/The cloud dataset\"\n",
    "path_out = abs_path+\"/P3_Testing\"\n",
    "\n",
    "new_array, new_array_pca, new_array_border, new_array_border_pca = save_dataset(path_in, path_out, year=\"2018\", num_labels=4, slices=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41867af",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for k in range(1, 13):\n",
    "    if k <=9:\n",
    "        file_names = natsorted(os.listdir(abs_path+f\"/The cloud dataset/2017M0{k}\"))\n",
    "        files += [os.path.join(f\"{path_in}/2017M0{k}\", f) for f in file_names if f.endswith(\".npy\") and f != \"TIMESTAMPS.npy\"]\n",
    "    if k>=10:\n",
    "        file_names = natsorted(os.listdir(abs_path+f\"/The cloud dataset/2017M{k}\"))\n",
    "        files += [os.path.join(f\"{path_in}/2017M{k}\", f) for f in file_names if f.endswith(\".npy\") and f != \"TIMESTAMPS.npy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36249201",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d1 = xr.open_dataset(abs_path+\"/Files/new_array.nc\")['data'].values\n",
    "d2 = xr.open_dataset(abs_path+\"/Files/new_array_pca_fnormalized.nc\")['data'].values\n",
    "d3 = xr.open_dataset(abs_path+\"/Files/new_array_border.nc\")['data'].values\n",
    "d4 = xr.open_dataset(abs_path+\"/Files/new_array_border_pca_fnormalized.nc\")['data'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9aac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(4, 4)\n",
    "j=4000\n",
    "for k in range(4):\n",
    "    plt.subplot(4, 4, k+1)\n",
    "    plt.imshow(d1[j,k,:,:], cmap='gray')\n",
    "    plt.subplot(4, 4, k+5)\n",
    "    plt.imshow(d2[j,k,:,:], cmap='gray')\n",
    "    plt.subplot(4, 4, k+9)\n",
    "    plt.imshow(d3[j,k,:,:], cmap='gray')\n",
    "    plt.subplot(4, 4, k+13)\n",
    "    plt.imshow(d4[j,k,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd633f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "plt.get_current_fig_manager().full_screen_toggle()\n",
    "plt.savefig(abs_path+\"/12_new_4x_sliced_arrays.png\")\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975e01f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_toarray_selection(path_in, variable, num_labels=4):\n",
    "        \n",
    "    dataset = xr.open_dataset(path_in)\n",
    "    darray = dataset[variable].values\n",
    "    \n",
    "    len_time, num_labels, m, n = darray.shape\n",
    "\n",
    "    y = np.zeros((len_time*num_labels))\n",
    "    X = np.zeros((len_time*num_labels, m*n))\n",
    "        \n",
    "    for t in tqdm(range(len_time*num_labels)):\n",
    "        X[t, :] = darray[t//num_labels, t%num_labels, :, :].flatten()\n",
    "        y[t] = t%num_labels\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def run_datasetNN(files, variable=\"data\", hidden_layer_size=32, num_labels=4, itterations=100):\n",
    "    \n",
    "    X, y = data_toarray_selection(files, variable, num_labels)\n",
    "    print(\"Training dataset of size : \", X.shape)\n",
    "\n",
    "    Theta1, Theta2 = ML.nnOneLayer(X, y, hidden_layer_size, num_labels, itterations)\n",
    "        \n",
    "    return Theta1, Theta2\n",
    "\n",
    "\n",
    "def test_datasetNN(files, Theta1, Theta2, variable=\"data\", sample_size=100, num_labels=13):\n",
    "    \n",
    "    X, y = data_toarray_selection(files, variable, num_labels)\n",
    "    p_arrays = ML.predOneLayer(X, Theta1, Theta2)\n",
    "    \n",
    "    len_sample, len_data = X.shape\n",
    "    print(\"Testing dataset of size : \", X.shape)\n",
    "    \n",
    "    for k in range(num_labels):\n",
    "        success_of_label_k = 0\n",
    "        for t in range(k, len_sample, num_labels):\n",
    "            if p_arrays[t] == k:\n",
    "                success_of_label_k+=1\n",
    "        success_of_label_k = trunc(success_of_label_k/(len_sample/num_labels)*10000)\n",
    "        print(\"Success of label\", k, \"is : \", success_of_label_k/100, \"%\")\n",
    "\n",
    "    return p_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9ae0a",
   "metadata": {
    "title": "Computation of images NN"
   },
   "outputs": [],
   "source": [
    "files = abs_path+\"/P3_Training/new_array_pca.nc\"\n",
    "\n",
    "Theta1, Theta2 = run_datasetNN(files, variable=\"data\", hidden_layer_size=32, num_labels=4, itterations=100)\n",
    "\n",
    "np.save(abs_path+\"/P3_Training/trained_theta_NN_new_array_pca.npy\", np.array([Theta1, Theta2], dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702af58f",
   "metadata": {
    "title": "Testing of images NN, training dataset"
   },
   "outputs": [],
   "source": [
    "files = abs_path+\"/P3_Training/new_array_pca.nc\"\n",
    "Theta1, Theta2 = np.load(abs_path+\"/P3_Training/trained_theta_NN_new_array_pca.npy\", allow_pickle=True)\n",
    "\n",
    "p_arrays = test_datasetNN(files, Theta1, Theta2, variable=\"data\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79bdb6c",
   "metadata": {
    "title": "Testing of images NN, testing dataset"
   },
   "outputs": [],
   "source": [
    "files = abs_path+\"/P3_Testing/new_array_pca.nc\"\n",
    "Theta1, Theta2 = np.load(abs_path+\"/P3_Training/trained_theta_NN_new_array_pca.npy\", allow_pickle=True)\n",
    "\n",
    "p_arrays = test_datasetNN(files, Theta1, Theta2, variable=\"data\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533ca2c",
   "metadata": {
    "title": "Computation of border NN"
   },
   "outputs": [],
   "source": [
    "files = abs_path+\"/P3_Training/new_array_border_pca.nc\"\n",
    "\n",
    "Theta1_, Theta2_ = run_datasetNN(files, variable=\"data\", hidden_layer_size=32, num_labels=13, itterations=100)\n",
    "\n",
    "np.save(abs_path+\"/P3_Training/trained_theta_NN_new_array_border_pca.npy\", np.array([Theta1_, Theta2_], dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c12572",
   "metadata": {
    "title": "Testing of border NN, training dataset"
   },
   "outputs": [],
   "source": [
    "files = abs_path+\"/P3_Training/new_array_border_pca.nc\"\n",
    "Theta1_, Theta2_ = np.load(abs_path+\"/P3_Training/trained_theta_NN_new_array_border_pca.npy\", allow_pickle=True)\n",
    "\n",
    "p_arrays = test_datasetNN(files, Theta1_, Theta2_, variable=\"data\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b81357",
   "metadata": {
    "title": "Testing of border NN, testing dataset"
   },
   "outputs": [],
   "source": [
    "files = abs_path+\"/P3_Testing/new_array_border_pca.nc\"\n",
    "Theta1_, Theta2_ = np.load(abs_path+\"/P3_Training/trained_theta_NN_new_array_border_pca.npy\", allow_pickle=True)\n",
    "\n",
    "p_arrays = test_datasetNN(files, Theta1_, Theta2_, variable=\"data\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623bb7f8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#================ END OF PART III ================#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
