{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ PART IV ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e75896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import convolve2d\n",
    "from natsort import natsorted\n",
    "from math import trunc\n",
    "\n",
    "import ML\n",
    "\n",
    "\"\"\"\n",
    "Specify the absolute path of the 'Project' folder location :\n",
    "\"\"\"\n",
    "abs_path = \"D:/Machine Learning/Project\"\n",
    "\n",
    "#The cloud dataset, 6go, np arrays 30+go\n",
    "#https://www.kaggle.com/datasets/christianlillelund/the-cloudcast-dataset?resource=download\n",
    "#https://vision.eng.au.dk/cloudcast-dataset/\n",
    "\n",
    "#The cloud dataset : dataset of suboptimal npy arrays to dataarray\n",
    "\n",
    "#Data is structured in 24 files, for every month, named {year}M{month}\n",
    "#Every npy is the labelized cloud category, at t={num_array}*15 minutes\n",
    "#The data is studied above europe, centered on France, and covers some parts of the Maghreb\n",
    "#More especially between the long and lats : cf GEO.npz\n",
    "\n",
    "TestCloud = xr.open_dataset(abs_path+\"/small_cloud/TestCloud.nc\")[\"__xarray_dataarray_variable__\"]\n",
    "TrainingCloud = xr.open_dataset(abs_path+\"/small_cloud/TrainCloud.nc\")[\"__xarray_dataarray_variable__\"]\n",
    "\n",
    "array_info = TrainingCloud.values[:,:,0]\n",
    "array_info[array_info==255] = 4\n",
    "\n",
    "m, n, len_time = TrainingCloud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88324e78",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Categories are initially as followed :\n",
    "    1-3 and 255 represent land caracteristics, and are thus considered zeros\n",
    "    4-8 are very low, low, mid, high and very high clouds\n",
    "    9 are fractionnal clouds\n",
    "    10-13 are semitransparent high clouds\n",
    "Categories will be merged as :\n",
    "    0 land caracteristics\n",
    "    1-5 very low, low, mid, high and very high clouds\n",
    "    6 fractionnal and semi transparent high clouds\n",
    "\"\"\"\n",
    "\n",
    "plt.hist(TrainingCloud.values[:,:,0].ravel(), bins=12, range=(1, 13))\n",
    "plt.title(\"Pixels distribution by category before merging\")\n",
    "plt.savefig(abs_path+\"/15_pixels_histogram_before_merging.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e99fcd3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def label_merging(dataset):\n",
    "    data = dataset.copy()\n",
    "    m, n, len_time = data.shape\n",
    "    \n",
    "    data_hist = data.values[:,:,0].ravel()\n",
    "    for t in tqdm(range(len_time)):\n",
    "        array = data[:,:,t].values\n",
    "        #No cloud\n",
    "        array[array==255] = 0\n",
    "        array[array<=3] = 0\n",
    "        #Low clouds\n",
    "        array[array==4] = 1\n",
    "        array[array==5] = 1\n",
    "        #Mid clouds\n",
    "        array[array==6] = 2\n",
    "        #High clouds\n",
    "        array[array>=7] = 3\n",
    "        data[:,:,t] = array\n",
    "            \n",
    "    plt.hist(data.values.ravel(), bins=4, range=(0, 4))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5515c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = abs_path+\"/small_cloud/TrainCloud_merged.nc\"\n",
    "data_training = label_merging(TrainingCloud)\n",
    "data_training.to_netcdf(path_out)\n",
    "\n",
    "array = data_training[:,:,0]\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(array, cmap='cool')\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(array.values.flatten(), bins=4, range=(0, 4))\n",
    "plt.suptitle(\"Pixels count distribution, eg1\")\n",
    "plt.savefig(abs_path+\"/16_pixels_categories_distribution_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b58ea6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "path_out = abs_path+\"/small_cloud/TestCloud_merged.nc\"\n",
    "data_test = label_merging(TestCloud)\n",
    "data_test.to_netcdf(path_out)\n",
    "\n",
    "array = data_test[:,:,1000]\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(array, cmap='cool')\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(array.values.flatten(), bins=4, range=(0, 4))\n",
    "plt.suptitle(\"Pixels count distribution, eg2\")\n",
    "plt.savefig(abs_path+\"/17_pixels_categories_distribution_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6da992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ PCA new_dataset : Small Cloud ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_array(array, k_main_components, normalization=True):\n",
    "    \"\"\"\n",
    "    Module fonction.\n",
    "    Runs PCA algorithm on an input array and keep the first k_main_components.\n",
    "    Input array can be normalized or not.\n",
    "    \"\"\"\n",
    "    array_pca, pca_info = ML.pca(array, k_main_components, normalization)\n",
    "    return array_pca, pca_info\n",
    "\n",
    "\n",
    "def label_merging_pca(array, num_labels=4, k_main_components=30):\n",
    "        \n",
    "    m, n = array.shape\n",
    "    \n",
    "    kernel = np.array([[-1, -1, -1], [-1,  1, -1], [-1, -1, -1]])\n",
    "    \n",
    "    new_array = np.zeros((num_labels, m, n))\n",
    "    new_array_pca = np.zeros((num_labels, m, k_main_components))\n",
    "    for k in range(num_labels):\n",
    "        array_label = np.zeros((m,n))\n",
    "        array_label[array==k] = 1\n",
    "        array_label[array!=k] = 0\n",
    "\n",
    "        new_array[k, :, :] = array_label\n",
    "        new_array_pca[k,:,:] = pca_array(array_label, k_main_components)[0]\n",
    "    \n",
    "    new_array_border = np.zeros((num_labels, m, n))\n",
    "    new_array_border_pca = np.zeros((num_labels, m, k_main_components))\n",
    "    for k in range(num_labels):\n",
    "        new_array_border[k,:,:] = convolve2d(new_array[k,:,:], kernel, mode='same', boundary='symm')\n",
    "        new_array_border_pca[k,:,:] = pca_array(new_array_border[k,:,:], k_main_components)[0]\n",
    "        \n",
    "    return new_array, new_array_pca, new_array_border, new_array_border_pca\n",
    "\n",
    "\n",
    "def save_dataset(path_in, path_out, variable=\"__xarray_dataarray_variable__\", num_labels=4, k_main_components=30, compression=False):\n",
    "    \"\"\"\n",
    "    Save all the arrays in path_in folder into path_out folder, as merged datasets alongside dimension time.\n",
    "    Calls the function merged_arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = xr.open_dataset(path_in)[variable]\n",
    "    m, n, len_time = dataset.shape\n",
    "\n",
    "    list_new_array, list_new_array_pca, list_new_array_border, list_new_array_border_pca = [], [], [], []\n",
    "    for t in tqdm(range(0, len_time, 8)):\n",
    "        new_array, new_array_pca, new_array_border, new_array_border_pca = label_merging_pca(dataset[:,:,t], num_labels, k_main_components)\n",
    "        \n",
    "        list_new_array.append(xr.DataArray(new_array, dims=('label', 'x', 'y')))\n",
    "        list_new_array_pca.append(xr.DataArray(new_array_pca, dims=('label', 'x', 'y')))\n",
    "        list_new_array_border.append(xr.DataArray(new_array_border, dims=('label', 'x', 'y')))\n",
    "        list_new_array_border_pca.append(xr.DataArray(new_array_border_pca, dims=('label', 'x', 'y')))\n",
    "    \n",
    "    len_dataset = len(list_new_array)\n",
    "    \n",
    "    d1 = xr.DataArray(list_new_array, \n",
    "                 dims=('time', 'label', 'x', 'y'), \n",
    "                 coords={'time': range(len_dataset)},\n",
    "                 name='data')\n",
    "    d1.to_netcdf(f\"{path_out}/new_array.nc\", \n",
    "                            mode='w', format='netCDF4', engine='netcdf4', encoding={'data': {'zlib': compression}})\n",
    "    \n",
    "    d2 = xr.DataArray(list_new_array_pca, \n",
    "                 dims=('time', 'label', 'x', 'y'), \n",
    "                 coords={'time': range(len_dataset)},\n",
    "                 name='data')\n",
    "    d2.to_netcdf(f\"{path_out}/new_array_pca.nc\", \n",
    "                            mode='w', format='netCDF4', engine='netcdf4', encoding={'data': {'zlib': compression}})\n",
    "    \n",
    "    d3 = xr.DataArray(list_new_array_border, \n",
    "                 dims=('time', 'label', 'x', 'y'), \n",
    "                 coords={'time': range(len_dataset)},\n",
    "                 name='data')\n",
    "    d3.to_netcdf(f\"{path_out}/new_array_border.nc\", \n",
    "                            mode='w', format='netCDF4', engine='netcdf4', encoding={'data': {'zlib': compression}})\n",
    "    \n",
    "    d4 = xr.DataArray(list_new_array_border_pca, \n",
    "                 dims=('time', 'label', 'x', 'y'), \n",
    "                 coords={'time': range(len_dataset)},\n",
    "                 name='data')\n",
    "    d4.to_netcdf(f\"{path_out}/new_array_border_pca.nc\", \n",
    "                            mode='w', format='netCDF4', engine='netcdf4', encoding={'data': {'zlib': compression}})\n",
    "    \n",
    "    return new_array, new_array_pca, new_array_border, new_array_border_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6302711",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = abs_path+\"/small_cloud/TrainingCloud.nc\"\n",
    "path_out = abs_path+\"/small_cloud/k10\"\n",
    "\n",
    "new_array, new_array_pca, new_array_border, new_array_border_pca = save_dataset(path_in, path_out, \n",
    "                                                                                num_labels=4, k_main_components=10, compression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = abs_path+\"/small_cloud/TestCloud.nc\"\n",
    "path_out = abs_path+\"/small_cloud/Testing_k30\"\n",
    "\n",
    "test_array, test_array_pca, test_array_border, test_array_border_pca = save_dataset(path_in, path_out, \n",
    "                                                                                    num_labels=4, k_main_components=30, compression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd728c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = xr.open_dataset(abs_path+\"/small_cloud/new_array.nc\")['data'].values\n",
    "d2 = xr.open_dataset(abs_path+\"/small_cloud/new_array_pca.nc\")['data'].values\n",
    "d3 = xr.open_dataset(abs_path+\"/small_cloud/new_array_border.nc\")['data'].values\n",
    "d4 = xr.open_dataset(abs_path+\"/small_cloud/new_array_border_pca.nc\")['data'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbd569e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "for k in range(4):\n",
    "    plt.subplot(2,2,k+1)\n",
    "    plt.imshow(d1[0,k,:,:], cmap='cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea258ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ Neural Network - 2 ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31bde0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_toarray_normalize_selection(path_in, variable, num_labels=4):\n",
    "        \n",
    "    dataset = xr.open_dataset(path_in)\n",
    "    darray = dataset[variable].values\n",
    "    \n",
    "    len_time, num_labels, m, n = darray.shape\n",
    "\n",
    "    y = np.zeros((len_time*num_labels))\n",
    "    X = np.zeros((len_time*num_labels, m*n))\n",
    "        \n",
    "    for t in tqdm(range(len_time*num_labels)):\n",
    "        X[t, :] = darray[t//num_labels, t%num_labels, :, :].flatten()\n",
    "        y[t] = t%num_labels\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def run_datasetNN(path_in, variable=\"data\", hidden_layer_size=32, num_labels=4, itterations=100):\n",
    "    \n",
    "    print(\"Loading the normalized X and y to prevent overflows :\")\n",
    "    X, y = data_toarray_normalize_selection(path_in, variable, num_labels)\n",
    "    print(\"Training dataset of size : \", X.shape)\n",
    "\n",
    "    Theta1, Theta2 = ML.nnOneLayer(X, y, hidden_layer_size, num_labels, itterations)\n",
    "        \n",
    "    return Theta1, Theta2, X, y\n",
    "\n",
    "\n",
    "def test_datasetNN(path_in, Theta1, Theta2, variable=\"data\", sample_size=100, num_labels=13):\n",
    "    \n",
    "    print(\"Loading and normalizing X_test and y_test to match the training examples :\")\n",
    "    X, y = data_toarray_normalize_selection(path_in, variable, num_labels)\n",
    "    \n",
    "    len_sample, len_data = X.shape\n",
    "    print(\"Testing dataset of size : \", X.shape)\n",
    "    \n",
    "    p_arrays = ML.predOneLayer(X, Theta1, Theta2)\n",
    "    \n",
    "    for k in range(num_labels):\n",
    "        success_of_label_k = 0\n",
    "        for t in range(k, len_sample, num_labels):\n",
    "            if p_arrays[t] == k:\n",
    "                success_of_label_k+=1\n",
    "        success_of_label_k = trunc(success_of_label_k/(len_sample/num_labels)*10000)\n",
    "        print(\"Success of label\", k, \"is : \", success_of_label_k/100, \"%\")\n",
    "\n",
    "    return p_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e20a07",
   "metadata": {
    "title": "Computation of images NN"
   },
   "outputs": [],
   "source": [
    "files = abs_path+\"/small_cloud/k30/new_array_pca.nc\"\n",
    "\n",
    "Theta1, Theta2, X, y = run_datasetNN(files, variable=\"data\", hidden_layer_size=32, num_labels=4, itterations=100)\n",
    "\n",
    "np.save(abs_path+\"/small_cloud/k30/trained_theta_NN_new_array_pca.npy\", np.array([Theta1, Theta2], dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bade866",
   "metadata": {
    "title": "Testing of images NN, training dataset"
   },
   "outputs": [],
   "source": [
    "files = abs_path+\"/small_cloud/k30/new_array_pca.nc\"\n",
    "Theta1, Theta2 = np.load(abs_path+\"/small_cloud/k30/trained_theta_NN_new_array_pca.npy\", allow_pickle=True)\n",
    "\n",
    "p_arrays = test_datasetNN(files, Theta1, Theta2, variable=\"data\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0c7ef",
   "metadata": {
    "title": "Testing of images NN, testing dataset"
   },
   "outputs": [],
   "source": [
    "files = abs_path+\"/small_cloud/Testing_k30/new_array_pca.nc\"\n",
    "Theta1, Theta2 = np.load(abs_path+\"/small_cloud/k30/trained_theta_NN_new_array_pca.npy\", allow_pickle=True)\n",
    "\n",
    "p_arrays = test_datasetNN(files, Theta1, Theta2, variable=\"data\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f69ce",
   "metadata": {
    "title": "Computation of border NN"
   },
   "outputs": [],
   "source": [
    "files = abs_path+\"/small_cloud/k30/new_array_border_pca.nc\"\n",
    "\n",
    "Theta1_, Theta2_, X, y = run_datasetNN(files, variable=\"data\", hidden_layer_size=32, num_labels=4, itterations=100)\n",
    "\n",
    "np.save(abs_path+\"/small_cloud/k30/trained_theta_NN_new_array_border_pca.npy\", np.array([Theta1_, Theta2_], dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388cb6cf",
   "metadata": {
    "title": "Testing of border NN, training dataset"
   },
   "outputs": [],
   "source": [
    "files = abs_path+\"/small_cloud/k30/new_array_border_pca.nc\"\n",
    "Theta1_, Theta2_ = np.load(abs_path+\"/small_cloud/k30/trained_theta_NN_new_array_border_pca.npy\", allow_pickle=True)\n",
    "\n",
    "p_arrays = test_datasetNN(files, Theta1_, Theta2_, variable=\"data\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f801621",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Testing of border NN, testing dataset"
   },
   "outputs": [],
   "source": [
    "files = abs_path+\"/small_cloud/Testing_k30/new_array_border_pca.nc\"\n",
    "Theta1_, Theta2_ = np.load(abs_path+\"/small_cloud/k30/trained_theta_NN_new_array_border_pca.npy\", allow_pickle=True)\n",
    "\n",
    "p_arrays = test_datasetNN(files, Theta1_, Theta2_, variable=\"data\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab254eb0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def imageAndBorders(path_in, path_out):\n",
    "    \n",
    "    ds1 = xr.open_dataset(f\"{path_in}/new_array_pca.nc\")\n",
    "    ds2 = xr.open_dataset(f\"{path_in}/new_array_border_pca.nc\")\n",
    "\n",
    "    # Concatenate the datasets along array axis=1\n",
    "    dataset_combined = xr.concat([ds1, ds2], dim='y')\n",
    "    \n",
    "    dataset_combined.to_netcdf(f\"{path_out}/new_combined_pca.nc\")\n",
    "    \n",
    "    return dataset_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcb6825",
   "metadata": {
    "title": "Concatenation of array and border datasets"
   },
   "outputs": [],
   "source": [
    "path_in = abs_path+\"/small_cloud\"\n",
    "path_out = abs_path+\"/small_cloud\"\n",
    "\n",
    "dataset_combined = imageAndBorders(path_in, path_out)\n",
    "darray_combined = dataset_combined[\"data\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a430b0a",
   "metadata": {
    "title": "Computation of array&border NN"
   },
   "outputs": [],
   "source": [
    "files = abs_path+\"/small_cloud/new_combined_pca.nc\"\n",
    "\n",
    "Theta1_, Theta2_, X, y = run_datasetNN(files, variable=\"data\", hidden_layer_size=32, num_labels=4, itterations=100)\n",
    "\n",
    "np.save(abs_path+\"/small_cloud/trained_theta_NN_new_combined_pca.npy\", np.array([Theta1_, Theta2_], dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b60dd07",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Testing of array&border NN, training dataset"
   },
   "outputs": [],
   "source": [
    "files = abs_path+\"/small_cloud/new_combined_pca.nc\"\n",
    "Theta1_, Theta2_ = np.load(abs_path+\"/small_cloud/trained_theta_NN_new_combined_pca.npy\", allow_pickle=True)\n",
    "\n",
    "p_arrays = test_datasetNN(files, Theta1_, Theta2_, variable=\"data\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f812e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ OneVsAll - 2 ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b0145",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def OneVsAll_dataset(path_in, variable, lambda_=1, num_labels=4, itterations=50):\n",
    "    \n",
    "    print(\"Loading of X and y :\")\n",
    "    X, y = data_toarray_normalize_selection(path_in, variable, num_labels)\n",
    "    \n",
    "    len_sample, len_data = X.shape\n",
    "    print(\"Testing dataset of size : \", X.shape)\n",
    "    \n",
    "    trained_theta = ML.oneVsAll(X, y, num_labels, lambda_, tol=1e-3)[1]\n",
    "    \n",
    "    return trained_theta, X, y\n",
    "\n",
    "\n",
    "def OneVsAll_testDataset_pca(path_in, trained_theta, variable, sample_size=30, num_labels=13):\n",
    "    \n",
    "    X, y = data_toarray_normalize_selection(path_in, variable, num_labels)\n",
    "    \n",
    "    len_time, m = X.shape\n",
    "    \n",
    "    p = ML.predictOneVsAll(trained_theta[:,1:], X)\n",
    "    \n",
    "    for k in range(num_labels):\n",
    "        success_of_label_k = 0\n",
    "        for t in range(k, len_time, num_labels):\n",
    "            if p[t] == k:\n",
    "                success_of_label_k+=1\n",
    "        success_of_label_k = success_of_label_k / (len_time/num_labels)\n",
    "        print(\"Success of label\", k, \"is : \", success_of_label_k*100, \"%\")\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed7ab4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Due to the computation time being very steep (30sec/itt for k=10, 4min/itt for k=20, 25min/itt for k=30), calculation \n",
    "can't be runned for k>=30, thus forcing the size of array&border combined dataset to be capped at 2*(k=10) = 20.\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc40a0",
   "metadata": {
    "title": "Computation of OneVsAll algorithm, image"
   },
   "outputs": [],
   "source": [
    "path_in = abs_path+\"/small_cloud/k30/new_array_pca.nc\"\n",
    "\n",
    "trained_theta, X, y = OneVsAll_dataset(path_in, variable=\"data\", lambda_=1, num_labels=4, itterations=50)\n",
    "\n",
    "np.save(abs_path+\"/small_cloud/trained_theta_OneVsAll_new_array_pca.npy\", trained_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee7442",
   "metadata": {
    "title": "Testing of OneVsAll algorithm, image trained and test dataset"
   },
   "outputs": [],
   "source": [
    "path_in = abs_path+\"/small_cloud/k30/new_array_pca.nc\"\n",
    "trained_theta = np.load(abs_path+\"/small_cloud/k30/trained_theta_OneVsAll_new_array_pca.npy\")\n",
    "\n",
    "p_arrays = OneVsAll_testDataset_pca(path_in, trained_theta, variable =\"data\", num_labels=4)\n",
    "\n",
    "path_in = abs_path+\"/small_cloud/Testing_k30/new_array_pca.nc\"\n",
    "\n",
    "p_arrays_test = OneVsAll_testDataset_pca(path_in, trained_theta, variable =\"data\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1080de",
   "metadata": {
    "title": "Computation of OneVsAll algorithm, border"
   },
   "outputs": [],
   "source": [
    "path_in = abs_path+\"/small_cloud/k30/new_array_border_pca.nc\"\n",
    "\n",
    "trained_theta, X, y = OneVsAll_dataset(path_in, variable=\"data\", lambda_=1, num_labels=4, itterations=50)\n",
    "\n",
    "np.save(abs_path+\"/small_cloud/k30/trained_theta_OneVsAll_new_array_border_pca.npy\", trained_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108d6e7",
   "metadata": {
    "title": "Testing of OneVsAll algorithm, border trained and test dataset"
   },
   "outputs": [],
   "source": [
    "path_in = abs_path+\"/small_cloud/k30/new_array_border_pca.nc\"\n",
    "trained_theta = np.load(abs_path+\"/small_cloud/k30/trained_theta_OneVsAll_new_array_border_pca.npy\")\n",
    "\n",
    "p_arrays = OneVsAll_testDataset_pca(path_in, trained_theta, variable =\"data\", num_labels=4)\n",
    "\n",
    "path_in = abs_path+\"/small_cloud/Testing_k30/new_array_border_pca.nc\"\n",
    "\n",
    "p_arrays_test = OneVsAll_testDataset_pca(path_in, trained_theta, variable =\"data\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8cce45",
   "metadata": {
    "title": "Computation of OneVsAll algorithm, combined dataset"
   },
   "outputs": [],
   "source": [
    "path_in = abs_path+\"/small_cloud/k10/new_combined_pca.nc\"\n",
    "\n",
    "trained_theta, X, y = OneVsAll_dataset(path_in, variable=\"data\", lambda_=1, num_labels=4, itterations=50)\n",
    "\n",
    "np.save(abs_path+\"/small_cloud/k10/trained_theta_OneVsAll_new_combined_pca.npy\", trained_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d00b46",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Testing of OneVsAll algorithm, combined dataset"
   },
   "outputs": [],
   "source": [
    "path_in = abs_path+\"/small_cloud/k10/new_combined_pca.nc\"\n",
    "trained_theta = np.load(abs_path+\"/small_cloud/k10/trained_theta_OneVsAll_new_combined_pca.npy\")\n",
    "\n",
    "p_arrays = OneVsAll_testDataset_pca(path_in, trained_theta, variable =\"data\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6295186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ What if it worked perfectly ? ================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef3e67",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "def kmeans(image_array, num_clusters, itterations=1000):\n",
    "    # Reshape the image array to a 2D array of pixels\n",
    "    pixels = image_array\n",
    "    \n",
    "    # Randomly initialize cluster centers\n",
    "    centers = pixels[np.random.choice(pixels.shape[0], num_clusters, replace=False)]\n",
    "    \n",
    "    # Iterate until convergence\n",
    "    for k in tqdm(range(itterations)):\n",
    "        # Assign pixels to the nearest cluster center\n",
    "        distances = np.linalg.norm(pixels[:, np.newaxis, :] - centers, axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Update cluster centers\n",
    "        new_centers = np.array([pixels[labels == i].mean(axis=0) for i in range(num_clusters)])\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.allclose(centers, new_centers):\n",
    "            break\n",
    "        \n",
    "        centers = new_centers\n",
    "    \n",
    "    # Assign pixel values to the closest cluster center\n",
    "    new_pixels = centers[labels]\n",
    "    \n",
    "    # Reshape the pixel values to match the original image shape\n",
    "    new_image = new_pixels.reshape(image_array.shape)\n",
    "    \n",
    "    return new_image\n",
    "\n",
    "num_clusters = 50\n",
    "image_array = xr.open_dataset(abs_path+\"/small_cloud/TrainCloud_merged.nc\")[\"__xarray_dataarray_variable__\"][:,:,0].values\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image_array)\n",
    "array = kmeans(image_array, num_clusters)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(array)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
