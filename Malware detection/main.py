
# %% IMPORTS
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from torchvision import transforms

import os
import sys

# The path from the root to the current file is retreived in main_path
main_path = os.path.abspath("")
input_data_path = main_path
output_model_path = main_path + "/Models"
print(f"\n> Root: {main_path}\n")

# %% DATA LOADING

# images_malware.npz has an object structure, and the data is stored in the "arr" file
data = np.load(input_data_path + "/images_malware.npz", allow_pickle=True)
data = data["arr"]

# data is a list of [ [datapoint1, label1], [datapoint2, labe2], ...]
# np.stack allows to cast a list to an array type, as if the values where "stacked" one above an other, alongside dim=1 (axis=1)
arrays = np.expand_dims(np.stack([datapoint[0] for datapoint in data]), axis=1)
# same goes for labels
labels = np.array([np.stack([datapoint[1] for datapoint in data])]).T
num_classes = np.max(labels) + 1 # +1 because 0 is a label
# Perform one-hot encoding
labels = np.squeeze(np.eye(num_classes)[labels])

# In torch, in_channels (1 for gray level, 3 for rgb) preceeds heigth and width
batch_size, in_channels, height, width = arrays.shape
output_size = np.shape(labels)[-1] # width of the one-hot encoded target labels

print("> batch_size, in_channels, height, width, output_size:")
print("\t", batch_size, "\t" ,in_channels, "\t", height, "\t", width, "\t", output_size)

# %% PREPROCESSING

print("> in_channels, output_size:\n", "\t", in_channels, "\t", output_size)

# Training and testing data splitting, 25% goes to testing sample 
X_train, X_test, Y_train, Y_test = train_test_split(arrays, labels, test_size=0.25)
print("> X_train.shape, Y_train.shape, X_test.shape, Y_test.shape:")
print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)

input_size = X_train.shape[0]

# The image data is normalized using the standard Z-score formula. 
# Pixels values could also be divided by 255, the impact is negligeable
mean = np.mean(arrays, axis=0)
std = np.std(arrays, axis=0)
# normalize is a torch.normalize object, that can be used in a data transformation pipeline
normalize = transforms.Normalize(mean=mean, std=std)

# The training data and labels are gathered into a TensorDataset object, that allows an easier preprocessing and can then be 
# sent into a dataloader object  
train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(Y_train).float())
# The dataset is normalized using the single pipeline object normalize
train_dataset.transform = transforms.Compose([normalize])
# The dataset is sent into a dataloader object for esaier processing
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Same applies to the testing data and labels
test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(Y_test).float())
test_dataset.transform = transforms.Compose([normalize])
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)

# %% MODEL
class CNN(nn.Module):
    def __init__(self, in_channels=1, output_size=25, filters=16, hidden_layer=128):
        super(CNN, self).__init__()
        
        self.in_channels = in_channels
        self.outputsize = output_size
        self.filters = filters
        self.hidden_layer = hidden_layer

        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=filters, kernel_size=3, padding=1, stride=1)
        self.conv2 = nn.Conv2d(in_channels=filters, out_channels=4*filters, kernel_size=3, padding=1, stride=1)

        self.linear1 = nn.Linear(self.num_flat_features(torch.randn(in_channels, filters, 32, 32)), self.hidden_layer)
        self.linear2 = nn.Linear(self.hidden_layer, out_features=output_size)

    def forward(self, x):
        
        # conv1 layer is applied onto the input data to extract features
        x = self.relu(self.conv1(x))

        # The maxpooling reduces the feature space size to the same dimension it was before the feature extraction
        x = self.maxpool(x)
        # An other round of features extraction
        x = self.relu(self.conv2(x))
        
        # The data is reshaped into [batch, flattened_extracted_features] array, to be then sorted into categories 
        x = x.view(-1, self.num_flat_features(x))

        # First dense layer, to reduce the number of features to sort
        x = self.linear1(x)
        x = self.relu(x)
        # Sorting layer into one hot encoded targets
        x = self.linear2(x)

        return x

    #Flattens along dim>=1, ie all but batch size
    def num_flat_features(self, x):
        size = x.size()[1:]
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


# %% MODEL TRAINING
if __name__ == "__main__" and "training" in sys.argv:

    model = CNN(in_channels=in_channels, 
                output_size=output_size,
    )
    model_name = "CNN"

    # a.k.a. categorical crossentropy, a loss function that already features a softmax activation layer to the one hot encoded output
    criterion = nn.CrossEntropyLoss()

    # default parameters
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    model.train() 

    running_loss = 0.0
    losses_list = []
    #num_epochs = int(input("Number of epochs : "))    
    num_epochs = int(input("Number of training epochs: ") or 100) # User input or 100 nb if epochs
    for epoch in tqdm(range(num_epochs)):
        # Standard torch training loop
        for inputs, targets in train_dataloader:

            optimizer.zero_grad()  
            outputs = model(inputs)  

            loss = criterion(outputs, targets)

            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            losses_list.append(loss.item()) # Losses history

    print(f"Final loss: {loss.item()}")

    plt.semilogy(losses_list)
    plt.title("Training losses")
    plt.xlabel("Epochs")
    plt.ylabel("Running loss")
    plt.savefig(f"{output_model_path}/Loss_{model_name}_{num_epochs}e.png")
    plt.show()

    # Model's weights are saved
    torch.save(model.state_dict(), f"{output_model_path}/{model_name}_{num_epochs}e.pth")
    print("Finished training, model saved")

# %% MODEL TESTING
if __name__ == "__main__" and "testing" in sys.argv:
    
    try:
        # if a model was trained during this script execution
        model = CNN(in_channels=in_channels, output_size=output_size)
        model.load_state_dict(torch.load(f"{output_model_path}/CNN_{num_epochs}e.pth"))
        model_loaded = "Currently trained model used "
    except:
        if not ("pretrained" in sys.argv):
            # if a model wasn't trained, and weights should be loaded
            num_epochs = int(input("Number of training epochs: ") or 100) # User input or 100 nb if epochs
            model = CNN(in_channels=in_channels, output_size=output_size)
            model.load_state_dict(torch.load(f"{output_model_path}/CNN_{num_epochs}e.pth"))
            model_loaded = f"{num_epochs}e model loaded"

    # Loading pre trained model if available
    if (False or ("pretrained" in sys.argv)):
        try:
            model.load_state_dict(torch.load(f"{output_model_path}/CNN_10e.pth"))
            model_loaded = "10 epochs pre trained model loaded"
        except:
            None
        try:
            model.load_state_dict(torch.load(f"{output_model_path}/CNN_100e.pth"))
            model_loaded = "100 epochs pre trained model loaded"
        except:
            None
        try:
            model.load_state_dict(torch.load(f"{output_model_path}/CNN_1000e.pth"))
            model_loaded = "1k epochs pre trained model loaded"
        except:
            None
        try:
            model.load_state_dict(torch.load(f"{output_model_path}/CNN_10000e.pth"))
            model_loaded = "10k epochs pre trained model loaded"
        except:
            None
        try:
            model.load_state_dict(torch.load(f"{output_model_path}/CNN_20000e.pth"))
            model_loaded = "20k epochs pre trained model loaded"
        except:
            None
    print(model_loaded)

    model.eval()
    with torch.no_grad():
        for test_features, test_labels in test_dataloader: # Tests real accuracy
            # Accuracy on test dataset

            test_labels = torch.argmax(test_labels, dim=1).numpy()

            test_predicted = torch.argmax(model(test_features), dim=1).numpy()

            # +1 if correct label found, +0 otherwise
            test_success = np.sum(np.equal(test_predicted, test_labels))
            print(f"testing accuracy: {test_success/test_predicted.shape[-1]:.4f}")

        for train_features, train_labels in train_dataloader: # Tests overfitting
            # If the model is pretrained, then is the same as test accuracy, because the datapoints are shuffled
             
            train_labels = torch.argmax(train_labels, dim=1).numpy()

            train_predicted = torch.argmax(model(train_features), dim=1).numpy()

            # +1 if correct label found, +0 otherwise
            train_success = np.sum(np.equal(train_predicted, train_labels))
            print(f"training accuracy: {train_success/train_predicted.shape[-1]:4f}")

    # predict_values, pred_counts = np.unique(test_predicted, return_counts=True)
    # labels_values, test_counts = np.unique(test_labels, return_counts=True)
    # print(pred_counts)
    # print(test_counts)


